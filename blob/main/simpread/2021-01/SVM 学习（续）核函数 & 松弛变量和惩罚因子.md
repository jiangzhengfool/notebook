> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [www.cnblogs.com](https://www.cnblogs.com/charlesblc/p/6243647.html)

SVM 的文章可以看：[http://www.cnblogs.com/charlesblc/p/6193867.html](http://www.cnblogs.com/charlesblc/p/6193867.html)

有写的最好的文章来自：[http://www.blogjava.net/zhenandaci/category/31868.html](http://www.blogjava.net/zhenandaci/category/31868.html)

这里面貌似也有一些机器学习文章：[http://leftnoteasy.cnblogs.com/](http://leftnoteasy.cnblogs.com/)

下面这个系列 [Jasper's Java Jacal](http://www.blogjava.net/zhenandaci/) 里面的 SVM 真的讲的好。已经把每一篇都下载了，目录：

/Users/baidu/Documents/Data/Interview / 机器学习 - 数据挖掘 / SVM from Jasper's Java Jacal

注：一到六的笔记在后面，可以按照需要的顺序看。

这一篇关于核函数的讲的非常好（七）
-----------------

[http://www.blogjava.net/zhenandaci/archive/2009/03/06/258288.html](http://www.blogjava.net/zhenandaci/archive/2009/03/06/258288.html)

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103122810034-1220270572.png)

这条曲线就是我们熟知的二次曲线，它的函数表达式可以写为：

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103122638659-1813711101.png)

这样 g(x) 就可以转化为 f(y)=<a,y>

看出妙在哪了么？原来在二维空间中一个线性不可分的问题，映射到四维空间后，变成了线性可分的！因此这也形成了我们最初想解决线性不可分问题的基本思路——向高维空间转化，使其变得线性可分。（注：四维的意思是，X 向量里有三个变量，+Y 这一个变量 = 4）

如果有这样的函数，那么当给了一个低维空间的输入 x 以后，

> g(x)=K(w,x)+b

> f(x’)=<w’,x’>+b

这两个函数的计算结果就完全一样，我们也就用不着费力找那个映射关系，直接拿低维的输入往 g(x) 里面代就可以了（再次提醒，这回的 g(x) 就不是线性函数啦，因为你不能保证 K(w,x) 这个表达式里的 x 次数不高于 1 哦）。

万幸的是，这样的 K(w,x) 确实存在（发现凡是我们人类能解决的问题，大都是巧得不能再巧，特殊得不能再特殊的问题，总是恰好有些能投机取巧的地方才能解决，由此感到人类的渺小），它被称作核函数（核，kernel），而且还不止一个，事实上，只要是满足了 Mercer 条件的函数，都可以作为核函数。核函数的基本作用就是接受两个低维空间里的向量，能够计算出经过某个变换后在高维空间里的向量内积值。

注：SVM 的主要核函数

```
多项式核: (gamma*u’*v + coef0)^degree

径向基核（RBF）: exp(-gamma*|u-v|^2)

Sigmoid 核: tanh(gamma*u’*v + coef0)
```

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103130242581-685960486.png)

看前面第五节，要求的一个线性分类器，它的形式应该是：

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103123244628-1298284666.png)

这个就是高维空间里的线性函数，我们就可以用一个低维空间里的函数（再一次的，这个低维空间里的函数就不再是线性的啦）来代替：

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103123343753-43436666.png)

f(x’) 和 g(x) 里的α，y，b 全都是一样一样的！这就是说，尽管给的问题是线性不可分的，但是我们就硬当它是线性问题来求解，只不过求解过程中，凡是要求内积的时候就用你选定的核函数来算。这样求出来的α再和你选定的核函数一组合，就得到分类器啦！

明白了以上这些，会自然的问接下来两个问题：

1． 既然有很多的核函数，针对具体问题该怎么选择？

2． 如果使用核函数向高维空间映射后，问题仍然是线性不可分的，那怎么办？

第一个问题现在就可以回答你：对核函数的选择，现在还缺乏指导原则！各种实验的观察结果（不光是文本分类）的确表明，某些问题用某些核函数效果很好，用另一些就很差，但是一般来讲，径向基核函数是不会出太大偏差的一种，首选。（我做文本分类系统的时候，使用径向基核函数，没有参数调优的情况下，绝大部分类别的准确和召回都在 85% 以上，可见。虽然 libSVM 的作者林智仁认为文本分类用线性核函数效果更佳，待考证）

对第二个问题的解决则引出了我们下一节的主题：松弛变量。

这一节对于松弛变量和惩罚因子讲的非常好（八）
----------------------

[http://www.blogjava.net/zhenandaci/archive/2009/03/15/259786.html](http://www.blogjava.net/zhenandaci/archive/2009/03/15/259786.html)

读者一定可以马上自己总结出来，松弛变量也就是个解决线性不可分问题的方法罢了，但是回想一下，核函数的引入不也是为了解决线性不可分的问题么？为什么要为了一个问题使用两种方法呢？

其实两者还有微妙的不同。一般的过程应该是这样，还以文本分类为例。在原始的低维空间中，样本相当的不可分，无论你怎么找分类平面，总会有大量的离群点，此时用核函数向高维空间映射一下，虽然结果仍然是不可分的，但比原始空间里的要更加接近线性可分的状态（就是达到了近似线性可分的状态），此时再用松弛变量处理那些少数 “冥顽不化” 的离群点，就简单有效得多啦。

本节中的（式 1）也确实是支持向量机最最常用的形式。至此一个比较完整的支持向量机框架就有了，简单说来，支持向量机就是使用了核函数的软间隔线性分类法。

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103112957800-425280590.png)

这个式子有这么几点要注意：

一是并非所有的样本点都有一个松弛变量与其对应。实际上只有 “离群点” 才有，或者也可以这么看，所有没离群的点松弛变量都等于 0。

二是松弛变量的值实际上标示出了对应的点到底离群有多远，值越大，点就越远。

三是惩罚因子 C 决定了你有多重视离群点带来的损失，显然当所有离群点的松弛变量的和一定时，你定的 C 越大，对目标函数的损失也越大，此时就暗示着你非常不愿意放弃这些离群点，最极端的情况是你把 C 定为无限大，这样只要稍有一个点离群，目标函数的值马上变成无限大，马上让问题变成无解，这就退化成了硬间隔问题。

四是惩罚因子 C 不是一个变量，整个优化问题在解的时候，C 是一个你必须事先指定的值，指定这个值以后，解一下，得到一个分类器，然后用测试数据看看结果怎么样，如果不够好，换一个 C 的值，再解一次优化问题，得到另一个分类器，再看看效果，如此就是一个参数寻优的过程，但这和优化问题本身决不是一回事，优化问题在解的过程中，C 一直是定值，要记住。

五是尽管加了松弛变量这么一说，但这个优化问题仍然是一个优化问题（汗，这不废话么），解它的过程比起原始的硬间隔问题来说，没有任何更加特殊的地方。

从大的方面说优化问题解的过程，就是先试着确定一下 w，也就是确定了前面图中的三条直线，这时看看间隔有多大，又有多少点离群，把目标函数的值算一算，再换一组三条直线（你可以看到，分类的直线位置如果移动了，有些原来离群的点会变得不再离群，而有的本来不离群的点会变成离群点），再把目标函数的值算一算，如此往复（迭代），直到最终找到目标函数最小时的 w。

继续（九）主要讲惩罚因子
------------

[http://www.blogjava.net/zhenandaci/archive/2009/03/17/260315.html](http://www.blogjava.net/zhenandaci/archive/2009/03/17/260315.html)

上一节的式子，是以前做 SVM 的人写的，大家也就这么用，但没有任何规定说必须对所有的松弛变量都使用同一个惩罚因子，我们完全可以给每一个离群点都使用不同的 C，这时就意味着你对每个样本的重视程度都不一样。

一种很常用的变形可以用来解决分类问题中样本的 “偏斜” 问题。

先来说说样本的偏斜问题，也叫数据集偏斜（unbalanced），它指的是参与分类的两个类别（也可以指多个类别）样本数量差异很大。比如说正类有 10，000 个样本，而负类只给了 100 个，这会引起的问题显而易见。

由于负类的样本很少很少，所以有一些本来是负类的样本点没有提供。

由于偏斜的现象存在，使得数量多的正类可以把分类面向负类的方向 “推”，因而影响了结果的准确性。

对付数据集偏斜问题的方法之一就是在惩罚因子上作文章，想必大家也猜到了，那就是给样本数量少的负类更大的惩罚因子。

libSVM 这个算法包在解决偏斜问题的时候用的就是这种方法。

那 C+ 和 C- 怎么确定呢？它们的大小是试出来的（参数调优），但是他们的比例可以有些方法来确定。咱们先假定说 C+ 是 5 这么大，那确定 C- 的一个很直观的方法就是使用两类样本数的比来算，对应到刚才举的例子，C- 就可以定为 500 这么大（因为 10，000：100=100：1 嘛）。

但是这样并不够好，给 C+ 和 C- 确定比例更好的方法应该是衡量他们分布的程度。比如可以算算他们在空间中占据了多大的体积，例如给负类找一个超球——就是高维空间里的球啦——它可以包含所有负类的样本，再给正类找一个，比比两个球的半径，就可以大致确定分布的情况。显然半径大的分布就比较广，就给小一点的惩罚因子。

但是这样还不够好，因为有的类别样本确实很集中，这个时候即便超球的半径差异很大，也不应该赋予两个类别不同的惩罚因子。完全的方法是没有的，根据需要，选择实现简单又合用的就好（例如 libSVM 就直接使用样本数量的比）。

多类分类（十）
-------

[http://www.blogjava.net/zhenandaci/archive/2009/03/26/262113.html](http://www.blogjava.net/zhenandaci/archive/2009/03/26/262113.html)

一种一劳永逸的方法，就是真的一次性考虑所有样本，并求解一个多目标函数的优化问题，一次性得到多个分类面。只可惜这种算法还基本停留在纸面上，因为一次性求解的方法计算量实在太大，大到无法实用的地步。

稍稍退一步，我们就会想到所谓 “一类对其余” 的方法。我们可以得到 5 个这样的两类分类器。我们就拿着这篇文章挨个分类器的问：是属于你的么？是属于你的么？哪个分类器点头说是了，文章的类别就确定了。

但有时也会出现两种很尴尬的情况，例如拿一篇文章问了一圈，每一个分类器都说它是属于它那一类的，或者每一个分类器都说它不是它那一类的，前者叫分类重叠现象，后者叫不可分类现象。

“其余”的那一类样本数总是要数倍于正类（因为它是除正类以外其他类别的样本之和嘛），这就人为的造成了上一节所说的 “数据集偏斜” 问题。

把一篇文章扔给所有分类器，第一个分类器会投票说它是 “1” 或者 “2”，第二个会说它是“1” 或者 “3”，让每一个都投上自己的一票，最后统计票数，如果类别“1” 得票最多，就判这篇文章属于第 1 类。这种方法显然也会有分类重叠的现象，但不会有不可分类现象，因为总不可能所有类别的票数都是 0。

类别数如果是 1000，要调用的分类器数目会上升至约 500,000 个（类别数的平方量级）。这如何是好？

在对一篇文章进行分类之前，我们先按照下面图的样子来组织分类器（如你所见，这是一个有向无环图，因此这种方法也叫做 DAG SVM）

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103125348581-122454844.png)

DAG 方法根节点的选取（也就是如何选第一个参与分类的分类器），我们总希望根节点少犯错误为好，因此参与第一次分类的两个类别，最好是差别特别特别大，大到以至于不太可能把他们分错；

或者我们就总取在两类分类中正确率最高的那个分类器作根节点，

或者我们让两类分类器在分类的时候，不光输出类别的标签，还输出一个类似 “置信度” 的东东，当它对自己的结果不太自信的时候，我们就不光按照它的输出走，把它旁边的那条路也走一走，等等。

**大 Tips****：SVM** **的计算复杂度**

使用 SVM 进行分类的时候，实际上是训练和分类两个完全不同的过程。我们这里所说的主要是**训练阶段的复杂度**，即解那个二次规划问题的复杂度。对这个问题的解，基本上要划分为两大块，解析解和数值解。

对 SVM 来说，求得解析解的时间复杂度最坏可以达到 O(Nsv3)，其中 Nsv 是支持向量的个数，而虽然没有固定的比例，但支持向量的个数多少也和训练集的大小有关。

数值解就是可以使用的解，是一个一个的数，往往都是近似解。求数值解的过程非常像穷举法，从一个数开始，试一试它当解效果怎样，不满足一定条件（叫做停机条件，就是满足这个以后就认为解足够精确了，不需要继续算下去了）就试下一个，当然下一个数不是乱选的，也有一定章法可循。有的算法，每次只尝试一个数，有的就尝试多个，而且找下一个数字（或下一组数）的方法也各不相同，停机条件也各不相同，最终得到的解精度也各不相同，可见对求数值解的复杂度的讨论不能脱开具体的算法。

一个具体的算法，Bunch-Kaufman 训练算法，典型的时间复杂度在 O(Nsv3+LNsv2+dLNsv) 和 O(dL2) 之间，其中 Nsv 是支持向量的个数，L 是训练集样本的个数，d 是每个样本的维数（原始的维数，没有经过向高维空间映射之前的维数）。复杂度会有变化，是因为它不光跟输入问题的规模有关（不光和样本的数量，维数有关），也和问题最终的解有关（即支持向量有关），如果支持向量比较少，过程会快很多，如果支持向量很多，接近于样本的数量，就会产生 O(dL2) 这个十分糟糕的结果（给 10，000 个样本，每个样本 1000 维，基本就不用算了，算不出来，呵呵，而这种输入规模对文本分类来说太正常了）。

这样再回头看就会明白为什么一对一方法尽管要训练的两类分类器数量多，但总时间实际上比一对其余方法要少了，因为一对其余方法每次训练都考虑了所有样本（只是每次把不同的部分划分为正类或者负类而已），自然慢上很多。

注：原系列到这里就没有再连载了。其实还是没有讲具体的解法。应该是具体的解法就跟数学解题又关了，比如上面提到的 Bunch-Kaufman 训练算法。这一部分可以再了解。

再回头复习一到六
--------

[http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html](http://www.blogjava.net/zhenandaci/archive/2009/02/13/254519.html)  一到三

支持向量机 (Support Vector Machine) 是 Cortes 和 Vapnik 于 1995 年首先提出的，它在解决小样本、非线性及高维模式识别中表现出许多特有的优势，并能够推广应用到函数拟合等其他机器学习问题中。

支持向量机方法是建立在统计学习理论的 VC 维理论和结构风险最小原理基础上的，根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度，Accuracy）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折衷，以期获得最好的推广能力 [14]（或称泛化能力）。

此时的情况便是选择了一个足够复杂的分类函数（它的 VC 维很高），能够精确的记住每一个样本，但对样本之外的数据一律分类错误。

统计学习因此而引入了泛化误差界的概念，就是指真实风险应该由两部分内容刻画，一是经验风险，代表了分类器在给定样本上的误差；二是置信风险，代表了我们在多大程度上可以信任分类器在未知文本上分类的结果。很显然，第二部分是没有办法精确计算的，因此只能给出一个估计的区间，也使得整个误差只能计算上界，而无法计算准确的值（所以叫做泛化误差界，而不叫泛化误差）。

置信风险与两个量有关，一是样本数量，显然给定的样本数量越大，我们的学习结果越有可能正确，此时置信风险越小；二是分类函数的 VC 维，显然 VC 维越大，推广能力越差，置信风险会变大。

统计学习的目标从经验风险最小化变为了寻求经验风险与置信风险的和最小，即结构风险最小。

非线性，是指 SVM 擅长应付样本数据线性不可分的情况，主要通过松弛变量（也有人叫惩罚变量）和核函数技术来实现，这一部分是 SVM 的精髓。

SVM 却可以，主要是因为 SVM 产生的分类器很简洁，用到的样本信息很少（仅仅用到那些称之为 “支持向量” 的样本，此为后话），使得即使样本维数很高，也不会给存储和计算带来大麻烦（相对照而言，kNN 算法在分类时就要用到所有样本，样本数巨大，每个样本维数再一高，这日子就没法过了……）。

（二）

中间那条直线的表达式是 g(x)=0，即 wx+b=0，我们也把这个函数叫做分类面。

（三）

我们可以让计算机这样来看待我们提供给它的训练样本，每一个样本由一个向量（就是那些文本特征所组成的向量）和一个标记（标示出这个样本属于哪个类别）组成。如下：

> Di=(xi,yi)

xi 就是文本向量（维数很高），yi 就是分类标记。

在二元的线性分类中，这个表示分类的标记只有两个值，1 和 - 1（用来表示属于还是不属于这个类）。有了这种表示法，我们就可以定义一个样本点到某个超平面的间隔：

> δi=yi(wxi+b)

注意，这个值其实等于 | wxi+b|

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103114736987-427033862.png)

小 Tips：||w|| 是什么符号？||w|| 叫做向量 w 的范数，范数是对向量长度的一种度量。我们常说的向量长度其实指的是它的 2 - 范数，范数最一般的表示形式为 p - 范数。

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103114842753-1406286229.png)

看看把 p 换成 2 的时候，不就是传统的向量长度么？当我们不指明 p 的时候，就像 ||w|| 这样使用时，就意味着我们不关心 p 的值，用几范数都可以；或者上文已经提到了 p 的值，为了叙述方便不再重复指明。

 当用归一化的 w 和 b 代替原值之后的间隔有一个专门的名称，叫做几何间隔，几何间隔所表示的正是点到超平面的欧氏距离，我们下面就简称几何间隔为 “距离”。以上是单个点到某个超平面的距离（就是间隔，后面不再区别这两个词）定义，同样可以定义一个点的集合（就是一组样本）到某个超平面的距离为此集合中离超平面最近的点的距离。

之所以如此关心几何间隔这个东西，是因为几何间隔与样本的误分次数间存在关系：

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103115148597-1591415589.png)

其中的δ是样本集合到分类面的间隔，R=max ||xi||  i=1,...,n，即 R 是所有样本中（xi 是以向量表示的第 i 个样本）向量长度最长的值（也就是说代表样本的分布有多么广）。

先不必追究误分次数的具体定义和推导过程，只要记得这个误分次数一定程度上代表分类器的误差。而从上式可以看出，误分次数的上界由几何间隔决定！（当然，是样本已知的时候）

至此我们就明白为何要选择几何间隔来作为评价一个解优劣的指标了，原来几何间隔越大的解，它的误差上界越小。因此最大化几何间隔成了我们训练阶段的目标，而且，与二把刀作者所写的不同，最大化分类间隔并不是 SVM 的专利，而是早在线性分类时期就已有的思想。

进一步深化（四）
--------

[http://www.blogjava.net/zhenandaci/archive/2009/02/13/254578.html](http://www.blogjava.net/zhenandaci/archive/2009/02/13/254578.html) 

上节说到我们有了一个线性分类函数，也有了判断解优劣的标准——即有了优化的目标，这个目标就是最大化几何间隔，但是看过一些关于 SVM 的论文的人一定记得什么优化的目标是要最小化 ||w|| 这样的说法，这是怎么回事呢？回头再看看我们对间隔和几何间隔的定义：

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103115607237-846119655.png)

可以看出δ=||w||δ几何。注意到几何间隔与 ||w|| 是成反比的，因此最大化几何间隔与最小化 ||w|| 完全是一回事。而我们常用的方法并不是固定 ||w|| 的大小而寻求最大几何间隔，而是固定间隔（例如固定为 1），寻找最小的 ||w||。

而凡是求一个函数的最小值（或最大值）的问题都可以称为寻优问题（也叫作一个规划问题）。

一个寻优问题最重要的部分是目标函数，顾名思义，就是指寻优的目标。例如我们想寻找最小的 ||w|| 这件事，就可以用下面的式子表示：

 ![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103115951034-343452963.png)

加平方是为了求导方便。

很容易看出当 ||w||=0 的时候就得到了目标函数的最小值。就是 H1 与 H2 两条直线间的距离无限大，这个时候，所有的样本点（无论正样本还是负样本）都跑到了 H1 和 H2 中间。所以要加上约束条件。

之前说了，把间隔固定为 1. 要让样本都在 H1 和 H2 两条线的某一侧。

满足这些条件就相当于让下面的式子总是成立：

    yi[(w·xi)+b]≥1 (i=1,2,…,l) （l 是总的样本数）

但我们常常习惯让式子的值和 0 比较，因而经常用变换过的形式：

    yi[(w·xi)+b]-1≥0 (i=1,2,…,l) （l 是总的样本数）

因此我们的两类分类问题也被我们转化成了它的数学形式，一个带约束的最小值的问题：

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103120230269-105457363.png)

 继续（五）
------

[http://www.blogjava.net/zhenandaci/archive/2009/02/14/254630.html](http://www.blogjava.net/zhenandaci/archive/2009/02/14/254630.html)

注意寻优问题，或者叫作规划（programming, 动态规划其实也是寻优问题）

概念：

可行域：要求 f(x) 在哪一点上取得最小值（反倒不太关心这个最小值到底是多少，关键是哪一点），但不是在整个空间里找，而是在约束条件所划定的一个有限的空间里找，这个有限的空间就是优化理论里所说的可行域。注意可行域中的每一个点都要求满足所有 p+q 个条件，而不是满足其中一条或几条就可以（切记，要满足每个约束），同时可行域边界上的点有一个额外好的特性，它们可以使不等式约束取得等号！而边界内的点不行。

凸集：关于可行域还有个概念不得不提，那就是凸集，凸集是指有这么一个点的集合，其中任取两个点连一条直线，这条线上的点仍然在这个集合内部，因此说 “凸” 是很形象的（一个反例是，二维平面上，一个月牙形的区域就不是凸集，你随便就可以找到两个点违反了刚才的规定）。

回头再来看我们线性分类器问题的描述，可以看出更多的东西。

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103120923769-474862034.png)

在这个问题中，自变量就是 w，而目标函数是 w 的二次函数，所有的约束条件都是 w 的线性函数（哎，千万不要把 xi 当成变量，它代表样本，是已知的），这种规划问题有个很有名气的称呼——二次规划（Quadratic Programming，QP），而且可以更进一步的说，由于它的可行域是一个凸集，因此它是一个凸二次规划。

```
一下子提了这么多术语，实在不是为了让大家以后能向别人炫耀学识的渊博，这其实是我们继续下去的一个重要前提，因为在动手求一个问题的解之前（好吧，我承认，是动计算机求……），我们必须先问自己：这个问题是不是有解？如果有解，是否能找到？
```

凸二次规划让人喜欢的地方就在于，它有解，而且可以找到！

我们可以轻松的解一个不带任何约束的优化问题（实际上就是当年背得烂熟的函数求极值嘛，求导再找 0 点呗，谁不会啊？笑），我们甚至还会解一个只带等式约束的优化问题，也是背得烂熟的，求条件极值，记得么，通过添加拉格朗日乘子，构造拉格朗日函数，来把这个问题转化为无约束的优化问题云云（如果你一时没想通，我提醒一下，构造出的拉格朗日函数就是转化之后的问题形式，它显然没有带任何条件）。

求条件极值方法可见：[Link](http://baike.baidu.com/link?url=CVwPORHM1FwbX1eIHcNLP4WZ-pYlrpdve1RdFu8-7mZgh1Z5m9_fPT0OxgBevtR8LfE1eTuhr6p0gGdScCDSkQFKjdqnPfn9BsZwikVVmQLn_mDg24G0dhOi3JcOQjnjDUrAA8xYKX9hLhMyncmO_a3wfJnkRImc30JjdtArhHi)

那么可不可以把带不等式约束的问题向只带等式约束的问题转化一下而得以求解呢？

聪明，可以，实际上我们也正是这么做的。

继续深入（六）
-------

[http://www.blogjava.net/zhenandaci/archive/2009/03/01/257237.html](http://www.blogjava.net/zhenandaci/archive/2009/03/01/257237.html)

我们想求得这样一个线性函数（在 n 维空间中的线性函数）：

g(x)=wx+b

使得所有属于正类的点 x+ 代入以后有 g(x+)≥1，而所有属于负类的点 x- 代入后有 g(x-)≤-1.

求这样的 g(x) 的过程就是求 w（一个 n 维向量）和 b（一个实数）两个参数的过程（但实际上只需要求 w，求得以后找某些样本点代入就可以求得 b）。因此在求 g(x) 的时候，w 才是变量。

样本确定了 w，用数学的语言描述，就是 w 可以表示为样本的某种组合：

w=α1x1+α2x2+…+αnxn

式子中的αi 是一个一个的数（在严格的证明过程中，这些α被称为拉格朗日乘子）

我会用α1x1 表示数字和向量的乘积，而用 < x1,x2> 表示向量 x1,x2 的内积（也叫点积，注意与向量叉积的区别）。因此 g(x) 的表达式严格的形式应该是：

g(x)=<w,x>+b

但是上面的式子还不够好, w 不仅跟样本点的位置有关，还跟样本的类别有关（也就是和样本的 “标签” 有关）。因此用下面这个式子表示才算完整：

w=α1y1x1+α2y2x2+…+αnynxn （式 1） 

其中的 yi 就是第 i 个样本的标签，它等于 1 或者 - 1。

以上式子的那一堆拉格朗日乘子中，只有很少的一部分不等于 0（不等于 0 才对 w 起决定作用），这部分不等于 0 的拉格朗日乘子后面所乘的样本点，其实都落在 H1 和 H2 上，也正是这部分样本（而不需要全部样本）唯一的确定了分类函数，当然，更严格的说，这些样本的一部分就可以确定，因为例如确定一条直线，只需要两个点就可以，即便有三五个都落在上面，我们也不是全都需要。这部分我们真正需要的样本点，就叫做支持（撑）向量！（名字还挺形象吧，他们 “撑” 起了分界线）

![](https://images2015.cnblogs.com/blog/899685/201701/899685-20170103122341737-1142726804.png)

以这样的形式描述问题以后，我们的优化问题少了很大一部分不等式约束（记得这是我们解不了极值问题的万恶之源）.